{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariantionalAutoencoder(object):\n",
    "    def __init__(self, latent_dim, image_dim=28 * 28):\n",
    "        self.hidden_encoder_dim = 256\n",
    "        self.hidden_decoder_dim = 256\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = 1e-2\n",
    "\n",
    "        self.image_dim = image_dim\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.x = tf.placeholder(name='x', dtype=tf.float32, shape=[None, self.image_dim])\n",
    "\n",
    "        # Gaussian MLP as encoder\n",
    "        with tf.variable_scope(\"gaussian_MLP_encoder\"):\n",
    "            he1 = tf.layers.dense(self.x, self.hidden_encoder_dim, activation=tf.nn.relu)\n",
    "            he2 = tf.layers.dense(he1, self.hidden_encoder_dim, activation=tf.nn.relu)\n",
    "            z_mu = tf.layers.dense(he2, self.latent_dim, activation=None)\n",
    "            z_log_sigma = tf.layers.dense(he2, self.latent_dim, activation=None)\n",
    "            self.z = ???\n",
    "\n",
    "        # Bernolli MLP as decoder\n",
    "        with tf.variable_scope(\"bernoulli_MLP_decoder\"):\n",
    "            hd1 = tf.layers.dense(self.z, self.hidden_decoder_dim, activation=tf.nn.relu)\n",
    "            hd2 = tf.layers.dense(hd1, self.hidden_decoder_dim, activation=tf.nn.relu)\n",
    "            self.x_hat = tf.layers.dense(hd2, self.image_dim, activation=tf.nn.sigmoid)\n",
    "\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            # Reconstruction loss\n",
    "            # Minimize the cross-entropy loss\n",
    "            # H(x, x_hat) = -\\Sigma x*log(x_hat) + (1-x)*log(1-x_hat)\n",
    "            recon_loss = ???\n",
    "            self.recon_loss = tf.reduce_mean(recon_loss)\n",
    "\n",
    "            # Latent loss\n",
    "            # Kullback Leibler divergence: measure the difference between two distributions\n",
    "            # Here we measure the divergence between the latent distribution and N(0, 1)\n",
    "            latent_loss = ???\n",
    "            self.latent_loss = tf.reduce_mean(latent_loss)\n",
    "            \n",
    "            self.total_loss = tf.reduce_mean(recon_loss + latent_loss)\n",
    "            self.train_op = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.learning_rate).minimize(self.total_loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "latent_dim = 100\n",
    "\n",
    "f_mnist = input_data.read_data_sets('../data/fashion_mnist')\n",
    "\n",
    "vae = VariantionalAutoencoder(latent_dim)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "for it in range(100000):\n",
    "    if it % 1000 == 0:\n",
    "        samples = sess.run(vae.x_hat, feed_dict={vae.z: sample_z(16, latent_dim)})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n",
    "\n",
    "    x_image, _ = f_mnist.train.next_batch(batch_size)\n",
    "\n",
    "    _, loss, recon_loss, latent_loss = sess.run(\n",
    "        [vae.train_op, vae.total_loss, vae.recon_loss, vae.latent_loss],\n",
    "        feed_dict={vae.x: x_image}\n",
    "    )\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('[Iter {}] Loss: {}, Recon loss: {}, Latent loss: {}'.format(\n",
    "            it, loss, recon_loss, latent_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
